{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, LayerNormalization, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load JSON data\n",
    "with open('pose_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Mapping exercise labels to numerical values\n",
    "exercise_labels = {exercise: idx for idx, exercise in enumerate(data.keys())}\n",
    "\n",
    "# Prepare data arrays\n",
    "X, y = [], []\n",
    "sequence_length = 30\n",
    "num_joints = len(next(iter(data.values()))[0][\"landmarks\"])  # Get the number of joints\n",
    "\n",
    "for exercise, frames in data.items():\n",
    "    for i in range(0, len(frames), sequence_length):\n",
    "        sequence = []\n",
    "        for frame_data in frames[i:i+sequence_length]:\n",
    "            landmarks = []\n",
    "            for joint in frame_data[\"landmarks\"]:\n",
    "                landmarks.extend([\n",
    "                    frame_data[\"landmarks\"][joint][\"x\"],\n",
    "                    frame_data[\"landmarks\"][joint][\"y\"],\n",
    "                    frame_data[\"landmarks\"][joint][\"z\"],\n",
    "                    frame_data[\"landmarks\"][joint][\"visibility\"]\n",
    "                ])\n",
    "            sequence.append(landmarks)\n",
    "        if len(sequence) == sequence_length:\n",
    "            X.append(sequence)\n",
    "            y.append(exercise_labels[exercise])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_37\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_37\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">267,264</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ bidirectional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,838</span> │ bidirectional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m132\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m267,264\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ bidirectional_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m263,168\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m164,352\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)        │      \u001b[38;5;34m2,838\u001b[0m │ bidirectional_3[\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">698,134</span> (2.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m698,134\u001b[0m (2.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">698,134</span> (2.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m698,134\u001b[0m (2.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.3201 - loss: 2.3242 - val_accuracy: 0.4341 - val_loss: 1.8090\n",
      "Epoch 2/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5034 - loss: 1.5356 - val_accuracy: 0.5995 - val_loss: 1.3929\n",
      "Epoch 3/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.5368 - loss: 1.4757 - val_accuracy: 0.5269 - val_loss: 1.4415\n",
      "Epoch 4/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.5633 - loss: 1.3657 - val_accuracy: 0.4583 - val_loss: 1.7379\n",
      "Epoch 5/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.4983 - loss: 1.5601 - val_accuracy: 0.4610 - val_loss: 1.7134\n",
      "Epoch 6/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.4985 - loss: 1.5373 - val_accuracy: 0.5444 - val_loss: 1.5809\n",
      "Epoch 7/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 122ms/step - accuracy: 0.4755 - loss: 1.6602 - val_accuracy: 0.4651 - val_loss: 1.8316\n",
      "Epoch 8/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.4708 - loss: 1.6367 - val_accuracy: 0.5484 - val_loss: 1.4554\n",
      "Epoch 9/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.5473 - loss: 1.3845 - val_accuracy: 0.4489 - val_loss: 1.6799\n",
      "Epoch 10/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.5326 - loss: 1.4865 - val_accuracy: 0.3750 - val_loss: 2.1929\n",
      "Epoch 11/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.4257 - loss: 1.7925 - val_accuracy: 0.3562 - val_loss: 2.0541\n",
      "Epoch 12/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.4304 - loss: 1.7760 - val_accuracy: 0.4973 - val_loss: 1.7084\n",
      "Epoch 13/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 109ms/step - accuracy: 0.5147 - loss: 1.5657 - val_accuracy: 0.4664 - val_loss: 1.5921\n",
      "Epoch 14/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 109ms/step - accuracy: 0.5114 - loss: 1.4932 - val_accuracy: 0.3360 - val_loss: 2.0431\n",
      "Epoch 15/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.4620 - loss: 1.6384 - val_accuracy: 0.4825 - val_loss: 1.7616\n",
      "Epoch 16/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.4745 - loss: 1.5743 - val_accuracy: 0.4852 - val_loss: 1.6086\n",
      "Epoch 17/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.4225 - loss: 1.8849 - val_accuracy: 0.3159 - val_loss: 2.1119\n",
      "Epoch 18/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.3708 - loss: 1.9866 - val_accuracy: 0.3992 - val_loss: 1.9059\n",
      "Epoch 19/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.4291 - loss: 1.7844 - val_accuracy: 0.4745 - val_loss: 1.7017\n",
      "Epoch 20/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.4804 - loss: 1.6261 - val_accuracy: 0.4355 - val_loss: 1.8649\n",
      "Epoch 21/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.5012 - loss: 1.4916 - val_accuracy: 0.4328 - val_loss: 1.9083\n",
      "Epoch 22/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 128ms/step - accuracy: 0.5177 - loss: 1.4412 - val_accuracy: 0.5202 - val_loss: 1.5284\n",
      "Epoch 23/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.5704 - loss: 1.2751 - val_accuracy: 0.5793 - val_loss: 1.3065\n",
      "Epoch 24/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.6346 - loss: 1.1349 - val_accuracy: 0.5215 - val_loss: 1.4425\n",
      "Epoch 25/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.5819 - loss: 1.2594 - val_accuracy: 0.6250 - val_loss: 1.2162\n",
      "Epoch 26/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 114ms/step - accuracy: 0.6254 - loss: 1.1312 - val_accuracy: 0.5430 - val_loss: 1.4290\n",
      "Epoch 27/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.5033 - loss: 1.4855 - val_accuracy: 0.4395 - val_loss: 1.8142\n",
      "Epoch 28/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.5376 - loss: 1.3841 - val_accuracy: 0.5685 - val_loss: 1.3829\n",
      "Epoch 29/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - accuracy: 0.5425 - loss: 1.4620 - val_accuracy: 0.5134 - val_loss: 1.6048\n",
      "Epoch 30/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.5436 - loss: 1.4142 - val_accuracy: 0.5524 - val_loss: 1.4123\n",
      "Epoch 31/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 135ms/step - accuracy: 0.6210 - loss: 1.1655 - val_accuracy: 0.5403 - val_loss: 1.4991\n",
      "Epoch 32/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 109ms/step - accuracy: 0.5758 - loss: 1.2882 - val_accuracy: 0.5121 - val_loss: 1.5912\n",
      "Epoch 33/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 114ms/step - accuracy: 0.5389 - loss: 1.4435 - val_accuracy: 0.4234 - val_loss: 2.0300\n",
      "Epoch 34/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 114ms/step - accuracy: 0.5466 - loss: 1.5040 - val_accuracy: 0.4879 - val_loss: 1.5784\n",
      "Epoch 35/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 129ms/step - accuracy: 0.5561 - loss: 1.3808 - val_accuracy: 0.5954 - val_loss: 1.3581\n",
      "Epoch 36/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 125ms/step - accuracy: 0.5206 - loss: 1.5281 - val_accuracy: 0.5336 - val_loss: 1.5460\n",
      "Epoch 37/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 118ms/step - accuracy: 0.4769 - loss: 1.6006 - val_accuracy: 0.4516 - val_loss: 1.7709\n",
      "Epoch 38/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 127ms/step - accuracy: 0.5610 - loss: 1.3631 - val_accuracy: 0.4906 - val_loss: 1.5228\n",
      "Epoch 39/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 138ms/step - accuracy: 0.5805 - loss: 1.2555 - val_accuracy: 0.5255 - val_loss: 1.5056\n",
      "Epoch 40/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.5093 - loss: 1.5656 - val_accuracy: 0.6169 - val_loss: 1.2484\n",
      "Epoch 41/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.6288 - loss: 1.1447 - val_accuracy: 0.5874 - val_loss: 1.3498\n",
      "Epoch 42/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.6199 - loss: 1.1466 - val_accuracy: 0.6102 - val_loss: 1.2734\n",
      "Epoch 43/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 139ms/step - accuracy: 0.6608 - loss: 1.0518 - val_accuracy: 0.6089 - val_loss: 1.3200\n",
      "Epoch 44/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.5292 - loss: 1.4670 - val_accuracy: 0.5645 - val_loss: 1.3791\n",
      "Epoch 45/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.6504 - loss: 1.0738 - val_accuracy: 0.5981 - val_loss: 1.2515\n",
      "Epoch 46/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.5068 - loss: 1.5032 - val_accuracy: 0.5228 - val_loss: 1.4423\n",
      "Epoch 47/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.5627 - loss: 1.3738 - val_accuracy: 0.5806 - val_loss: 1.3066\n",
      "Epoch 48/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.6086 - loss: 1.1917 - val_accuracy: 0.4825 - val_loss: 1.7344\n",
      "Epoch 49/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 113ms/step - accuracy: 0.5839 - loss: 1.3051 - val_accuracy: 0.5578 - val_loss: 1.3946\n",
      "Epoch 50/50\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.5630 - loss: 1.2606 - val_accuracy: 0.5887 - val_loss: 1.2911\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, Bidirectional, LSTM, Input, LayerNormalization, Dense, Dropout, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, num_joints * 4)\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# First Bidirectional LSTM Layer\n",
    "x = Bidirectional(LSTM(256, return_sequences=True))(input_layer)\n",
    "x = LayerNormalization()(x)  # Layer Normalization for stable training\n",
    "\n",
    "# MultiHeadAttention Layer with residual connection\n",
    "attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "x = Add()([x, attention_output])  # Residual connection\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Second Bidirectional LSTM Layer\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.3)(x)  # Dropout layer for regularization\n",
    "\n",
    "# Another Layer of MultiHeadAttention with residual connection\n",
    "attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "x = Add()([x, attention_output])  # Residual connection\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Final Bidirectional LSTM Layer\n",
    "x = Bidirectional(LSTM(64))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Dense layer with ReLU activation before the output layer\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer for classification\n",
    "output_layer = Dense(len(exercise_labels), activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with a custom learning rate\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5865 - loss: 1.2938\n",
      "Test Loss: 1.291059136390686\n",
      "Test Accuracy: 0.5887096524238586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose Estimator\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Function to process each frame and extract pose landmarks\n",
    "def extract_pose_landmarks_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB (MediaPipe uses RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame with MediaPipe Pose\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract landmarks for each joint\n",
    "            landmarks = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                landmarks.append({\n",
    "                    'x': landmark.x,\n",
    "                    'y': landmark.y,\n",
    "                    'z': landmark.z,\n",
    "                    'visibility': landmark.visibility\n",
    "                })\n",
    "            frames.append(landmarks)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Extract frames and pose landmarks from the video\n",
    "video_path = 'example_2.mp4'\n",
    "frames = extract_pose_landmarks_from_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'sequence_length' and 'exercise_labels' are already defined\n",
    "sequence_length = 30  # Length of the sequence to input to the model\n",
    "num_joints = 33  # Update this based on the number of joints in MediaPipe (or your dataset)\n",
    "\n",
    "def prepare_sequence_data(frames, sequence_length):\n",
    "    X = []\n",
    "    # Split frames into sequences of length 'sequence_length'\n",
    "    for i in range(0, len(frames) - sequence_length + 1, sequence_length):\n",
    "        sequence = []\n",
    "        for frame_data in frames[i:i+sequence_length]:\n",
    "            landmarks = []\n",
    "            for joint in frame_data:\n",
    "                landmarks.extend([joint[\"x\"], joint[\"y\"], joint[\"z\"], joint[\"visibility\"]])\n",
    "            sequence.append(landmarks)\n",
    "        X.append(sequence)\n",
    "    return np.array(X)\n",
    "\n",
    "# Prepare data for prediction\n",
    "X_video = prepare_sequence_data(frames, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict exercise using the trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_video)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert predictions to exercise labels\u001b[39;00m\n\u001b[0;32m      5\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict exercise using the trained model\n",
    "predictions = model.predict(X_video)\n",
    "\n",
    "# Convert predictions to exercise labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map predicted labels back to exercise names\n",
    "exercise_names = {idx: exercise for exercise, idx in exercise_labels.items()}\n",
    "predicted_exercise_name = exercise_names[predicted_labels[0]]  # Assuming one sequence is enough\n",
    "\n",
    "print(f\"Predicted exercise: {predicted_exercise_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model.save('exercise_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('exercise_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step\n",
      "Predicted exercise: plank\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('exercise_model.h5')\n",
    "\n",
    "# Initialize MediaPipe Pose Estimator\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Function to extract pose landmarks from a video\n",
    "def extract_pose_landmarks_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB (MediaPipe uses RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process frame with MediaPipe Pose\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract landmarks for each joint\n",
    "            landmarks = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                landmarks.append({\n",
    "                    'x': landmark.x,\n",
    "                    'y': landmark.y,\n",
    "                    'z': landmark.z,\n",
    "                    'visibility': landmark.visibility\n",
    "                })\n",
    "            frames.append(landmarks)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Prepare data for the model (same as during training)\n",
    "def prepare_sequence_data(frames, sequence_length):\n",
    "    X = []\n",
    "    for i in range(0, len(frames) - sequence_length + 1, sequence_length):\n",
    "        sequence = []\n",
    "        for frame_data in frames[i:i+sequence_length]:\n",
    "            landmarks = []\n",
    "            for joint in frame_data:\n",
    "                landmarks.extend([joint[\"x\"], joint[\"y\"], joint[\"z\"], joint[\"visibility\"]])\n",
    "            sequence.append(landmarks)\n",
    "        X.append(sequence)\n",
    "    return np.array(X)\n",
    "\n",
    "# Map predicted labels back to exercise names\n",
    "exercise_names = {idx: exercise for exercise, idx in exercise_labels.items()}\n",
    "\n",
    "# Video path\n",
    "video_path = 'example_4.mp4'\n",
    "\n",
    "# Extract pose data from the video\n",
    "frames = extract_pose_landmarks_from_video(video_path)\n",
    "\n",
    "# Prepare data for prediction\n",
    "sequence_length = 30  # Same as used during training\n",
    "X_video = prepare_sequence_data(frames, sequence_length)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_video)\n",
    "\n",
    "# Get the predicted labels (exercise categories)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the exercise name for the first prediction (or average over all predictions if needed)\n",
    "predicted_exercise_name = exercise_names[predicted_labels[0]]\n",
    "\n",
    "print(f\"Predicted exercise: {predicted_exercise_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('exercise_model.h5')\n",
    "\n",
    "# Initialize MediaPipe Pose Estimator\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Function to extract pose landmarks from a video\n",
    "def extract_pose_landmarks_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    # Initialize progress bar for video frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\", ncols=100) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convert frame to RGB (MediaPipe uses RGB)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process frame with MediaPipe Pose\n",
    "            results = pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Extract landmarks for each joint\n",
    "                landmarks = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    landmarks.append({\n",
    "                        'x': landmark.x,\n",
    "                        'y': landmark.y,\n",
    "                        'z': landmark.z,\n",
    "                        'visibility': landmark.visibility\n",
    "                    })\n",
    "                frames.append(landmarks)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Prepare data for the model (same as during training)\n",
    "def prepare_sequence_data(frames, sequence_length):\n",
    "    X = []\n",
    "    for i in range(0, len(frames) - sequence_length + 1, sequence_length):\n",
    "        sequence = []\n",
    "        for frame_data in frames[i:i+sequence_length]:\n",
    "            landmarks = []\n",
    "            for joint in frame_data:\n",
    "                landmarks.extend([joint[\"x\"], joint[\"y\"], joint[\"z\"], joint[\"visibility\"]])\n",
    "            sequence.append(landmarks)\n",
    "        X.append(sequence)\n",
    "    return np.array(X)\n",
    "\n",
    "# Load landmarks from JSON file\n",
    "def load_json_landmarks(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Debugging: Print the structure of the JSON to ensure it's correct\n",
    "    print(json.dumps(data, indent=4))  # Pretty print JSON\n",
    "    \n",
    "    processed_data = {}\n",
    "    for exercise_name, frames in data.items():\n",
    "        exercise_frames = []\n",
    "        for frame in frames:\n",
    "            landmarks_flat = []\n",
    "            # Debugging: Check the structure of the 'landmarks' field\n",
    "            if isinstance(frame, dict) and \"landmarks\" in frame:\n",
    "                for joint in frame[\"landmarks\"]:\n",
    "                    landmarks_flat.extend([joint[\"x\"], joint[\"y\"], joint[\"z\"], joint[\"visibility\"]])\n",
    "                exercise_frames.append(landmarks_flat)\n",
    "            else:\n",
    "                print(f\"Invalid frame structure: {frame}\")\n",
    "        processed_data[exercise_name] = exercise_frames\n",
    "    return processed_data\n",
    "\n",
    "# Calculate Mean Squared Error between two sets of landmarks\n",
    "def calculate_mse(video_landmarks, json_landmarks):\n",
    "    video_landmarks = np.array(video_landmarks)\n",
    "    json_landmarks = np.array(json_landmarks)\n",
    "\n",
    "    # Ensure both landmarks have the same shape\n",
    "    if video_landmarks.shape != json_landmarks.shape:\n",
    "        print(f\"Video landmarks shape: {video_landmarks.shape}\")\n",
    "        print(f\"JSON landmarks shape: {json_landmarks.shape}\")\n",
    "        raise ValueError(\"Landmark shapes do not match!\")\n",
    "\n",
    "    mse = np.mean(np.square(video_landmarks - json_landmarks))\n",
    "    return mse\n",
    "\n",
    "# Video path\n",
    "video_path = 'example_1.mp4'\n",
    "\n",
    "# Load JSON landmarks\n",
    "json_path = 'pose_data.json'  # Path to your JSON file\n",
    "json_data = load_json_landmarks(json_path)\n",
    "\n",
    "# Extract pose data from the video\n",
    "frames = extract_pose_landmarks_from_video(video_path)\n",
    "\n",
    "# Prepare data for prediction\n",
    "sequence_length = 30  # Same as used during training\n",
    "X_video = prepare_sequence_data(frames, sequence_length)\n",
    "\n",
    "# Make predictions\n",
    "if X_video.shape[0] > 0:\n",
    "    predictions = model.predict(X_video)\n",
    "\n",
    "    # Get the predicted labels (exercise categories)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Get the exercise name for the first prediction (assuming exercise names are known)\n",
    "    exercise_name = list(json_data.keys())[predicted_labels[0]]  # Map to exercise name\n",
    "    print(f\"Predicted exercise: {exercise_name}\")\n",
    "\n",
    "    # Extract corresponding JSON landmarks for the predicted exercise\n",
    "    json_landmarks_sequence = json_data[exercise_name]  # Get landmarks for the predicted exercise\n",
    "\n",
    "    # Ensure we have the same number of frames in both video and JSON\n",
    "    num_frames = min(len(X_video), len(json_landmarks_sequence))\n",
    "    \n",
    "    # Prepare to calculate MSE for each frame\n",
    "    total_mse = 0\n",
    "    with tqdm(total=num_frames, desc=\"Calculating MSE\", ncols=100) as pbar:\n",
    "        for i in range(num_frames):\n",
    "            video_landmarks = X_video[i]  # Get the ith sequence of landmarks\n",
    "            json_landmarks_flat = json_landmarks_sequence[i]  # Get the ith set of landmarks\n",
    "\n",
    "            # Calculate MSE for the current frame\n",
    "            try:\n",
    "                loss = calculate_mse(video_landmarks, json_landmarks_flat)\n",
    "                total_mse += loss\n",
    "                print(f\"Mean Squared Error for frame {i}: {loss}\")\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate average MSE over all frames\n",
    "    average_mse = total_mse / num_frames if num_frames > 0 else 0\n",
    "    print(f\"Average Mean Squared Error over {num_frames} frames: {average_mse}\")\n",
    "else:\n",
    "    print(\"No video data available for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and process JSON landmarks\n",
    "def load_json_landmarks(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Process the data\n",
    "    processed_data = {}\n",
    "    for exercise_name, frames in data.items():\n",
    "        exercise_frames = []\n",
    "        for frame in frames:\n",
    "            landmarks_flat = []\n",
    "            \n",
    "            # Flatten the 'landmarks' dictionary into a single list\n",
    "            if isinstance(frame, dict) and \"landmarks\" in frame:\n",
    "                landmarks = frame[\"landmarks\"]\n",
    "                for joint_name, joint_data in landmarks.items():\n",
    "                    # For each joint, extract the x, y, z, and visibility values\n",
    "                    landmarks_flat.extend([\n",
    "                        joint_data[\"x\"],\n",
    "                        joint_data[\"y\"],\n",
    "                        joint_data[\"z\"],\n",
    "                        joint_data[\"visibility\"]\n",
    "                    ])\n",
    "                exercise_frames.append(landmarks_flat)\n",
    "            else:\n",
    "                print(f\"Invalid frame structure: {frame}\")\n",
    "        \n",
    "        processed_data[exercise_name] = exercise_frames\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Example usage\n",
    "json_path = 'pose_data.json'  # Path to your JSON file\n",
    "json_data = load_json_landmarks(json_path)\n",
    "\n",
    "# Example of the structure of the loaded data\n",
    "print(json.dumps(json_data, indent=4))  # Pretty print JSON for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
